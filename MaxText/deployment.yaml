apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  # Modify this name to distinguish your workload from others.
  name: bernardhan-gcs-training-workload
  labels:
    # kueue.x-k8s.io/queue-name: multislice-queue  # Name of the LocalQueue
    xpk.google.com/workload: bernardhan-gcs-training-workload
  annotations:
    alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepool # 1:1 job replica to node pool assignment
spec:
  failurePolicy:
    maxRestarts: 0
  replicatedJobs:
    - name: warmup-job
      replicas: 1
      template:
        spec:
          parallelism: 512    # Equal to the number of VMs per slice
          completions: 512    # Same as the above.
          backoffLimit: 0   # When any pod fails, the job is failed
          template:
            metadata:
              labels:
                xpk.google.com/workload: bernardhan-gcs-training-workload
            spec:
              schedulerName: default-scheduler
              restartPolicy: Never
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: cloud.google.com/gke-nodepool
                        operator: NotIn
                        values:
                        - default-pool

              priorityClassName: medium
              hostNetwork: true
              dnsPolicy: ClusterFirstWithHostNet
              terminationGracePeriodSeconds: 30
              # For GCSFuse: the setup of K8S SA is needed. https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver#authentication
              # For other storage solutions that do not need the K8S SA, please remove this line.
              serviceAccountName: bernardhan-benchmark
              containers:
              - name: jax-cpu
                image: gcr.io/gcs-tess/bernardhan_training_gcsfs

                env:
                - name: REPLICATED_JOB_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['jobset.sigs.k8s.io/replicatedjob-name']
                - name: JOB_INDEX
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['jobset.sigs.k8s.io/job-index']
                - name: JOB_COMPLETION_INDEX
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
                # Modify the following two values too, if you intend to run the workload in smaller scale.
                - name: PROCESSES_IN_JOB
                  value: "512"
                - name: JAX_PROCESS_COUNT
                  value: "512"
                - name: JOBSET_NAME
                  value: "bernardhan-gcs-training-workload"
                - name: JAX_COORDINATOR_ADDRESS
                  value: "$(JOBSET_NAME)-$(REPLICATED_JOB_NAME)-0-0.$(JOBSET_NAME)"

                ports:
                - containerPort: 8471
                - containerPort: 8080
                - containerPort: 1234
                securityContext:
                  privileged: true
                command:
                - bash
                - -c
                - |
                  # Modify the parameters here.
                  export RUN_NAME=bernardhan-test-gcsfs
                  # export DATASET_DIRECTORY="/mnt/gcsfuse"
                  export DATASET_BUCKET="xai-hf-dataset-parquet-10g"
                  export EPOCHS=1
                  export MAX_STEPS=100
                  export LOCAL_BATCH_SIZE=1024
                  export PREFETCH_FACTOR=10
                  export DATA_LOADER_NUM_WORKERS=16
                  export PER_STEP_INTERVAL=0.1
                  export DATA_LOADER_STRATEGY_NAME="BaselineFileParallelSequentialRead"
                  export GCS_METRICS_BUCKET="distributed-training-metrics"

                  # Not recommended to modify the flags below.
                  export COMMON_RUN_FLAGS="enable_checkpointing=False hardware=cpu";
                  export BENCHMARK_RUN_FLAGS="run_name=${RUN_NAME} dataset_directory=${DATASET_DIRECTORY} dataset_bucket=${DATASET_BUCKET} epochs=${EPOCHS} max_steps=${MAX_STEPS} local_batch_size=${LOCAL_BATCH_SIZE} prefetch_factor=${PREFETCH_FACTOR} data_loader_num_workers=${DATA_LOADER_NUM_WORKERS} per_step_interval=${PER_STEP_INTERVAL} data_loader_strategy_name=${DATA_LOADER_STRATEGY_NAME} gcs_metrics_bucket=${GCS_METRICS_BUCKET}";
                  echo XPK Start: $(date) ; _sigterm() ( kill -SIGTERM $! 2>/dev/null;); trap _sigterm SIGTERM;(JAX_PLATFORMS=cpu python3 MaxText/standalone_dataloader.py MaxText/configs/base.yml ${BENCHMARK_RUN_FLAGS} ${COMMON_RUN_FLAGS}) & PID=$!; while kill -0 $PID 2>/dev/null; do sleep 5; done; wait $PID; EXIT_CODE=$? ;  echo XPK End: $(date); echo EXIT_CODE=$EXIT_CODE;